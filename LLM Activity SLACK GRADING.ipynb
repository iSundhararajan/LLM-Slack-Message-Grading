{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c46654e",
   "metadata": {},
   "source": [
    "\n",
    "## Scenario\n",
    "We decided to approach the activity by focusing on sentiment analysis but instead of 'positive' or 'negative' tweets we focused on determining whether a Slack message is 'Urgent', 'Not urgent' or 'Ambiguous'.\n",
    "\n",
    "In our scenario we imagined this tool would be used by someone in a large company that has many people reporting to them via Slack on a regular basis. This user wants to make sure Slack is not a hinderance/distraction to their workday and want to be efficient with their time. \n",
    "\n",
    "The model will analyze the sentiment of the messages received in Slack and grade whether the user should address them soon ('Urgent') or if it's alright to address the message later ('Not urgent') or if it falls in a middle ground ('ambiguous')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f81c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "api_key = open('key.txt','r').read().strip('\\n')\n",
    "\n",
    "\n",
    "\n",
    "# load and set our key\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f67f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "\n",
    "def gpt_response(inp, message_history, **params):\n",
    "    # We save the user's input\n",
    "    message_history.append({\"role\": \"user\", \"content\": f\"{inp}\"})\n",
    "    # Generate a response from the chatbot model\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model = 'gpt-3.5-turbo',\n",
    "      messages=message_history, \n",
    "      **params\n",
    "    )\n",
    "    # We save the assistant response\n",
    "    message_history.append({\"role\": \"assistant\", \"content\": f\"{completion.choices[0].message.content}\"})\n",
    "    return message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ca2865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changed temp to 1 but overall still did well. We did a good job of giving the system clear instructions from the beginning.\n",
    "\n",
    "message_history=[{\"role\": \"system\", \"content\": \"You are a marketing expert for a fortune 500 company.\" \n",
    "                  \"Decide whether a Slack message is urgent, not urgent or ambiguous based on it's relation to your ability to do your marketing job.\"},\n",
    "             {\"role\": \"user\", \"content\": \"Slack: \\\"I need you to create a presentation asap!\\\"\\nSentiment:\"},\n",
    "              {\"role\": \"assistant\", \"content\": \"Urgent\"}\n",
    "  ]\n",
    "print(\"Write a Slack message: \")\n",
    "\n",
    "while(True):\n",
    "    message_history = gpt_response(input(\"> \"),\n",
    "                                   message_history,\n",
    "                                   temperature=1,\n",
    "                                   max_tokens=60,\n",
    "                                   frequency_penalty=0.5)\n",
    "    print(message_history[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8185027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a Slack message: \n",
      "> \"Urgent: The ad copy has been rejected, please revise ASAP.\"\n",
      "Urgent\n",
      "> \"The ad copy has been rejected, please revise ASAP.\"\n",
      "Urgent\n",
      "> There's a team lunch next Wednesday. RSVP if you haven't already.\n",
      "Not urgent\n",
      "> Does anyone have a charger I can borrow?\n",
      "Not urgent\n",
      "> Question: a trade organization wants to sponsor some brands inside our program, and I need help figuring out the billing of it. Do we just create a 2 part payment plan and charge the sponsor for the enrollment prices regardless of the brandâ€™s participation? What am I missing here?\n",
      "Ambiguous\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a Slack message: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 11\u001b[0m     message_history \u001b[38;5;241m=\u001b[39m gpt_response(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m> \u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     12\u001b[0m                                    message_history,\n\u001b[0;32m     13\u001b[0m                                    temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     14\u001b[0m                                    max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m,\n\u001b[0;32m     15\u001b[0m                                    frequency_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(message_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1267\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "#alternate with no frequency penalty and removing \"based on it's relation to your ability to do your marketing job\"\n",
    "# responses are more generic and more black and white, less likely to mark the 'ambiguous' messages.\n",
    "message_history=[{\"role\": \"system\", \"content\": \"You are a marketing expert for a fortune 500 company. Decide whether a Slack message is urgent, not urgent or ambiguous.\"},\n",
    "             {\"role\": \"user\", \"content\": \"Slack message: \\\"I need you to create a presentation asap!\\\"\\nSentiment:\"},\n",
    "              {\"role\": \"assistant\", \"content\": \"Urgent\"}\n",
    "  ]\n",
    "print(\"Write a Slack message: \")\n",
    "\n",
    "while(True):\n",
    "    message_history = gpt_response(input(\"> \"),\n",
    "                                   message_history,\n",
    "                                   temperature=0,\n",
    "                                   max_tokens=60,\n",
    "                                   frequency_penalty=0.0)\n",
    "    print(message_history[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "191a545b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a Slack message: \n",
      "> \"Urgent: The ad copy has been rejected, please revise ASAP.\"\n",
      "Urgent\n",
      "> \"The ad copy has been rejected, please revise ASAP.\"\n",
      "Urgent\n",
      "> There's a team lunch next Wednesday. RSVP if you haven't already.\n",
      "Not urgent\n",
      "> Does anyone have a charger I can borrow?\n",
      "Not urgent\n",
      "> Question: a trade organization wants to sponsor some brands inside our program, and I need help figuring out the billing of it. Do we just create a 2 part payment plan and charge the sponsor for the enrollment prices regardless of the brandâ€™s participation? What am I missing here?\n",
      "Ambiguous\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a Slack message: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 11\u001b[0m     message_history \u001b[38;5;241m=\u001b[39m gpt_response(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m> \u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     12\u001b[0m                                    message_history,\n\u001b[0;32m     13\u001b[0m                                    temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;66;03m#Controls the randomness of the output.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m                                    max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, \u001b[38;5;66;03m#Limits the maximum number of tokens in the output.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m                                    frequency_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;66;03m#Penalizes new tokens based on their existing frequency in the text so far.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(message_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1267\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "#this is our best model, we have specific instructions for the system and the parameters set.\n",
    "\n",
    "message_history=[{\"role\": \"system\", \"content\": \"You are a marketing expert for a fortune 500 company. Decide whether a Slack message is urgent, not urgent or ambiguousbased on it's relation to your ability to do your marketing job.\"},\n",
    "             {\"role\": \"user\", \"content\": \"Slack message: \\\"I need you to create a presentation asap!\\\"\\nSentiment:\"},\n",
    "              {\"role\": \"assistant\", \"content\": \"Urgent\"}\n",
    "  ]\n",
    "print(\"Write a Slack message: \")\n",
    "\n",
    "while(True):\n",
    "    message_history = gpt_response(input(\"> \"),\n",
    "                                   message_history,\n",
    "                                   temperature=0, #Controls the randomness of the output.\n",
    "                                   max_tokens=12, #Limits the maximum number of tokens in the output.\n",
    "                                   frequency_penalty=0.5) #Penalizes new tokens based on their existing frequency in the text so far.\n",
    "    print(message_history[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48f387b",
   "metadata": {},
   "source": [
    "\n",
    "## Findings and Lessons learned\n",
    "- The more specific we were with our prompt for the system, the better and more consistent results we obtained.\n",
    "- Since we were focusing on sentiment analysis,temperature was the most important parameter since we did not want any random results. max_tokens and frequency_penalty were less significant than if we were looking for a generative response\n",
    "- we wanted to try using text-davinci-003 but were unable to because it has been deprecated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
